"""
Perception Server - FastAPI + Depth Anything V2
ç”¨é€”: æ¥æ”¶åœ–ç‰‡ï¼Œå›å‚³æ·±åº¦ä¼°è¨ˆèˆ‡é¿éšœå»ºè­°
"""

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
import torch
import numpy as np
from PIL import Image
import io
import sys
import os
import time

# åŠ å…¥ Depth Anything V2 metric_depth è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'metric_depth'))
from depth_anything_v2.dpt import DepthAnythingV2

app = FastAPI(
    title="Perception Server",
    description="DA3 æ·±åº¦ä¼°è¨ˆ API - ç‚º Go2 æ©Ÿå™¨ç‹—æä¾›é¿éšœæ„ŸçŸ¥",
    version="1.0.0"
)

# å…¨åŸŸæ¨¡å‹ç‰©ä»¶
model = None
device = None

# è·é›¢æ ¡æ­£ä¿‚æ•¸ (TODO: ç”¨ Go2 å¯¦æ‹æ ¡æ­£)
SCALE_FACTOR = 1.0


@app.on_event("startup")
async def load_models():
    """ä¼ºæœå™¨å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹"""
    global model, device
    
    print("ğŸ”„ æ­£åœ¨è¼‰å…¥ Depth Anything V2 æ¨¡å‹...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"  è£ç½®: {device}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = DepthAnythingV2(
        encoder='vitl',
        features=256,
        out_channels=[256, 512, 1024, 1024],
        max_depth=20.0
    )
    
    # è¼‰å…¥æ¬Šé‡
    checkpoint_path = os.path.join(
        os.path.dirname(__file__), 
        '..', 
        'checkpoints', 
        'depth_anything_v2_metric_hypersim_vitl.pth'
    )
    
    if not os.path.exists(checkpoint_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ¬Šé‡æª”æ¡ˆ: {checkpoint_path}")
        print("  è«‹å…ˆä¸‹è¼‰ metric depth æ¬Šé‡ï¼")
        return
    
    model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)
    model = model.to(device).eval()
    
    vram = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0
    print(f"âœ… DA3 æ¨¡å‹è¼‰å…¥å®Œæˆ (VRAM: {vram:.2f} GB)")


def analyze_depth_regions(depth_map: np.ndarray) -> dict:
    """
    åˆ†ææ·±åº¦åœ–çš„å·¦/ä¸­/å³å€åŸŸ
    
    Args:
        depth_map: æ·±åº¦åœ– (H, W)ï¼Œå–®ä½ç‚ºå…¬å°º
        
    Returns:
        å€åŸŸåˆ†æçµæœ dict
    """
    h, w = depth_map.shape
    
    # åˆ†å‰²ç‚ºå·¦ä¸­å³ä¸‰å€ï¼ˆå„ä½”ç•«é¢ 1/3ï¼‰
    left_region = depth_map[:, :w//3]
    center_region = depth_map[:, w//3:2*w//3]
    right_region = depth_map[:, 2*w//3:]
    
    # å–å„å€åŸŸçš„ä¸­ä½æ•¸è·é›¢ï¼ˆæ¯”å¹³å‡å€¼æ›´ç©©å¥ï¼‰
    left_dist = float(np.median(left_region)) * SCALE_FACTOR
    center_dist = float(np.median(center_region)) * SCALE_FACTOR
    right_dist = float(np.median(right_region)) * SCALE_FACTOR
    
    # å–ä¸­å¤®ä¸‹åŠéƒ¨ï¼ˆæ›´æº–ç¢ºåæ˜ å‰æ–¹éšœç¤™ç‰©ï¼‰
    center_lower = depth_map[h//2:, w//3:2*w//3]
    center_front = float(np.percentile(center_lower, 25)) * SCALE_FACTOR  # å–è¼ƒè¿‘çš„è·é›¢
    
    return {
        "left_m": round(left_dist, 2),
        "center_m": round(center_dist, 2),
        "right_m": round(right_dist, 2),
        "front_obstacle_m": round(center_front, 2),
        "min_m": round(float(np.min(depth_map)) * SCALE_FACTOR, 2),
        "max_m": round(float(np.max(depth_map)) * SCALE_FACTOR, 2)
    }


def generate_suggestion(regions: dict, obstacle_threshold: float = 1.0) -> str:
    """
    æ ¹æ“šå€åŸŸåˆ†æç”¢ç”Ÿé¿éšœå»ºè­°
    
    Args:
        regions: å€åŸŸåˆ†æçµæœ
        obstacle_threshold: éšœç¤™ç‰©è­¦æˆ’è·é›¢ï¼ˆå…¬å°ºï¼‰
        
    Returns:
        é¿éšœå»ºè­°å­—ä¸²
    """
    front = regions["front_obstacle_m"]
    left = regions["left_m"]
    right = regions["right_m"]
    
    if front > obstacle_threshold:
        return f"âœ… å‰æ–¹æš¢é€šï¼ˆ{front:.1f}mï¼‰ï¼Œå¯å®‰å…¨å‰é€²"
    
    # å‰æ–¹æœ‰éšœç¤™ï¼Œåˆ¤æ–·ç¹è¡Œæ–¹å‘
    if right > left and right > obstacle_threshold:
        return f"âš ï¸ æ­£å‰æ–¹ {front:.1f}m æœ‰éšœç¤™ï¼Œå»ºè­°å‘å³ç¹è¡Œï¼ˆå³å´ {right:.1f}m è¼ƒç©ºæ› ï¼‰"
    elif left > obstacle_threshold:
        return f"âš ï¸ æ­£å‰æ–¹ {front:.1f}m æœ‰éšœç¤™ï¼Œå»ºè­°å‘å·¦ç¹è¡Œï¼ˆå·¦å´ {left:.1f}m è¼ƒç©ºæ› ï¼‰"
    else:
        return f"ğŸ›‘ ä¸‰é¢å—é˜»ï¼ˆå‰ {front:.1f}m / å·¦ {left:.1f}m / å³ {right:.1f}mï¼‰ï¼Œå»ºè­°å¾Œé€€æˆ–åœæ­¢"


@app.get("/")
async def root():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "ok",
        "model_loaded": model is not None,
        "device": str(device) if device else "unknown"
    }


@app.post("/perceive")
async def perceive(
    image: UploadFile = File(...),
    obstacle_threshold: float = 1.0
):
    """
    æ¥æ”¶åœ–ç‰‡ï¼Œå›å‚³æ·±åº¦åˆ†æèˆ‡é¿éšœå»ºè­°
    
    Args:
        image: ä¸Šå‚³çš„åœ–ç‰‡æª”æ¡ˆ
        obstacle_threshold: éšœç¤™ç‰©è­¦æˆ’è·é›¢ï¼ˆå…¬å°ºï¼‰ï¼Œé è¨­ 1.0m
        
    Returns:
        JSON åŒ…å«å€åŸŸè·é›¢èˆ‡é¿éšœå»ºè­°
    """
    if model is None:
        return JSONResponse(
            status_code=503,
            content={"error": "æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆï¼Œè«‹ç¨å€™"}
        )
    
    start_time = time.time()
    
    try:
        # 1. è®€å–åœ–ç‰‡
        contents = await image.read()
        pil_image = Image.open(io.BytesIO(contents)).convert("RGB")
        raw_image = np.array(pil_image)
        
        # 2. æ¨è«–æ·±åº¦
        with torch.no_grad():
            depth_map = model.infer_image(raw_image)
        
        # 3. å€åŸŸåˆ†æ
        regions = analyze_depth_regions(depth_map)
        
        # 4. ç”¢ç”Ÿå»ºè­°
        suggestion = generate_suggestion(regions, obstacle_threshold)
        
        # 5. è¨ˆç®—è€—æ™‚
        elapsed_ms = (time.time() - start_time) * 1000
        
        return {
            **regions,
            "suggestion": suggestion,
            "inference_ms": round(elapsed_ms, 1),
            "image_size": f"{pil_image.width}x{pil_image.height}"
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"è™•ç†å¤±æ•—: {str(e)}"}
        )


@app.post("/perceive/summary")
async def perceive_summary(image: UploadFile = File(...)):
    """
    ç°¡åŒ–ç‰ˆï¼šåªå›å‚³ç´”æ–‡å­—æ„ŸçŸ¥æ‘˜è¦ï¼ˆçµ¦ LLM ä½¿ç”¨ï¼‰
    
    Returns:
        ç´”æ–‡å­—æ„ŸçŸ¥æ‘˜è¦
    """
    result = await perceive(image)
    
    if isinstance(result, JSONResponse):
        return result
    
    summary = f"""[ç’°å¢ƒæ„ŸçŸ¥æ‘˜è¦]
- å·¦å´: {result['left_m']}m
- æ­£å‰æ–¹: {result['front_obstacle_m']}m
- å³å´: {result['right_m']}m
{result['suggestion']}"""
    
    return {"summary": summary, "inference_ms": result["inference_ms"]}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
