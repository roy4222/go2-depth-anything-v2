# 感知融合系統簡報稿

## YOLO-World + Depth Anything V2 座標融合技術

---

## 📖 口述稿（搭配投影片使用）

### 【投影片 1：引言】

> 各位老師、同學好，接下來我要介紹的是我們專題中的**感知融合系統**。
>
> 簡單來說，這個系統讓機器狗能夠**同時辨識物體是什麼，以及它離我多遠**。

---

### 【投影片 2：問題與挑戰】

> 傳統做法會需要複雜的相機校正、3D 點雲投影、還有座標系轉換。
>
> 但我們用了一個**簡化的設計**：
> - 讓 YOLO-World 和 Depth Anything V2 **處理同一張圖片**
> - 這樣像素座標天然對齊，**不需要額外的座標變換**

---

### 【投影片 3：系統架構圖】（使用 `yolo_da2_fusion_zh.png`）

> 請看這張架構圖：
>
> 1. **左上角**是 YOLO-World 的物件偵測結果，它會給我們一個 bounding box，我們取中心點 `(cx, cy)`
>
> 2. **右上角**是 Depth Anything V2 產生的深度圖，每個像素都有一個深度值
>
> 3. **關鍵在於**：因為兩個模型吃的是同一張 640×480 的圖片，所以我們可以直接用 YOLO 的中心點座標，去查深度圖對應位置的距離
>
> 4. **下方**是後處理：我們會做分段校正來修正 DA2 的距離偏差，然後根據物體在畫面的左中右位置，決定導航方向

---

### 【投影片 4：核心程式碼】

> 讓我展示核心的幾行程式碼：
>
> ```python
> # YOLO 偵測 → 取得中心點
> cx = int((x1 + x2) / 2)
> cy = int((y1 + y2) / 2)
>
> # 用中心點查詢深度圖
> raw_distance = depth_map[cy, cx]
>
> # 分段校正
> distance = piecewise_distance_correction(raw_distance)
> ```
>
> 就這麼簡單！**三行程式碼完成座標融合**。

---

### 【投影片 5：分段距離校正】

> 我們在實測時發現，DA2 的原始輸出會偏大，而且不同距離的偏差程度不同：
>
> | 實際距離 | DA2 原始輸出 | 校正係數 |
> |---------|-------------|---------|
> | < 0.8m (近距離) | 偏差較小 | ×0.66 |
> | 0.8-3m (中距離) | 偏差較大 | ×0.23 |
> | > 3m (遠距離) | 偏差最大 | ×0.20 |
>
> 這個分段校正讓我們的距離估計誤差降到 **4.4%** 左右。

---

### 【投影片 6：方向判斷】

> 方向判斷就更直觀了：
>
> 我們把畫面分成三等份：
> - 物體在左邊 1/3 → 回報「左側」，機器狗左轉
> - 物體在中間 1/3 → 回報「正前方」，機器狗直行
> - 物體在右邊 1/3 → 回報「右側」，機器狗右轉

---

### 【投影片 7：API 輸出範例】

> 最後，我們的 `/find_object` API 會回傳這樣的結果：
>
> ```json
> {
>   "found": true,
>   "label": "water bottle",
>   "distance_m": 1.2,
>   "direction": "正前方",
>   "cmd_vel": {"linear_x": 0.2, "angular_z": 0.0}
> }
> ```
>
> LLM 或控制系統可以直接用這個結果來導航。

---

### 【投影片 8：效能數據】

> 整個推論流程的效能：
>
> | 項目 | 時間 |
> |------|------|
> | YOLO-World 偵測 | ~42 ms |
> | Depth Anything V2 推論 | ~289 ms |
> | **總計** | **~380 ms** |
>
> 在 GPU Server 上可以達到約 **2.5 FPS** 的處理速度，對於機器狗的導航應用來說是足夠的。

---

### 【投影片 9：總結】

> 總結一下這個設計的優點：
>
> 1. ✅ **簡單**：不需要相機內參、TF 變換、點雲投影
> 2. ✅ **準確**：分段校正讓距離誤差控制在 5% 以內
> 3. ✅ **即時**：380ms 的處理時間適合導航應用
> 4. ✅ **開放詞彙**：YOLO-World 可以偵測任意物體，不限於預訓練類別
>
> 以上就是我們感知融合系統的介紹，謝謝大家！

---

## 📊 Q&A 準備

### Q1：為什麼不用 3D 點雲做融合？

> 我們的機器狗使用無線連線，LiDAR 點雲的傳輸頻率只有 1-2 Hz，不適合即時融合。而且相機和 LiDAR 的座標轉換需要精確的外參校正。我們的方案用單一相機影像，避免了這些問題。

### Q2：分段校正的參數怎麼得到的？

> 我們實際測量了 15cm、30cm、45cm、1m 四個距離的 DA2 輸出值，然後計算各距離區間的校正係數。未來可以用更多數據點做回歸分析，得到更精確的校正曲線。

### Q3：遮擋怎麼處理？

> 目前我們取 bounding box 中心點的深度，如果物體被部分遮擋，可能會取到錯誤的深度值。改進方案是取 bounding box 內的深度中位數，而不是單一點。

---

## 🖼️ 搭配圖片

- `docs/yolo_da2_fusion_zh.png` - 系統架構圖（繁體中文版）
