<div align="center">
<h1>Depth Anything V2</h1>

[**Lihe Yang**](https://liheyoung.github.io/)<sup>1</sup> Â· [**Bingyi Kang**](https://bingykang.github.io/)<sup>2&dagger;</sup> Â· [**Zilong Huang**](http://speedinghzl.github.io/)<sup>2</sup>
<br>
[**Zhen Zhao**](http://zhaozhen.me/) Â· [**Xiaogang Xu**](https://xiaogang00.github.io/) Â· [**Jiashi Feng**](https://sites.google.com/site/jshfeng/)<sup>2</sup> Â· [**Hengshuang Zhao**](https://hszhao.github.io/)<sup>1*</sup>

<sup>1</sup>HKU&emsp;&emsp;&emsp;<sup>2</sup>TikTok
<br>
&dagger;project lead&emsp;*corresponding author

<a href="https://arxiv.org/abs/2406.09414"><img src='https://img.shields.io/badge/arXiv-Depth Anything V2-red' alt='Paper PDF'></a>
<a href='https://depth-anything-v2.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything V2-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-V2'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
<a href='https://huggingface.co/datasets/depth-anything/DA-2K'><img src='https://img.shields.io/badge/Benchmark-DA--2K-yellow' alt='Benchmark'></a>
</div>

This work presents Depth Anything V2. It significantly outperforms [V1](https://github.com/LiheYoung/Depth-Anything) in fine-grained details and robustness. Compared with SD-based models, it enjoys faster inference speed, fewer parameters, and higher depth accuracy.

![teaser](assets/teaser.png)


## News
- **2025-01-22:** [Video Depth Anything](https://videodepthanything.github.io) has been released. It generates consistent depth maps for super-long videos (e.g., over 5 minutes).
- **2024-12-22:** [Prompt Depth Anything](https://promptda.github.io/) has been released. It supports 4K resolution metric depth estimation when low-res LiDAR is used to prompt the DA models.
- **2024-07-06:** Depth Anything V2 is supported in [Transformers](https://github.com/huggingface/transformers/). See the [instructions](https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2) for convenient usage.
- **2024-06-25:** Depth Anything is integrated into [Apple Core ML Models](https://developer.apple.com/machine-learning/models/). See the instructions ([V1](https://huggingface.co/apple/coreml-depth-anything-small), [V2](https://huggingface.co/apple/coreml-depth-anything-v2-small)) for usage.
- **2024-06-22:** We release [smaller metric depth models](https://github.com/DepthAnything/Depth-Anything-V2/tree/main/metric_depth#pre-trained-models) based on Depth-Anything-V2-Small and Base.
- **2024-06-20:** Our repository and project page are flagged by GitHub and removed from the public for 6 days. Sorry for the inconvenience.
- **2024-06-14:** Paper, project page, code, models, demo, and benchmark are all released.


## Pre-trained Models

We provide **four models** of varying scales for robust relative depth estimation:

| Model | Params | Checkpoint |
|:-|-:|:-:|
| Depth-Anything-V2-Small | 24.8M | [Download](https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth?download=true) |
| Depth-Anything-V2-Base | 97.5M | [Download](https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth?download=true) |
| Depth-Anything-V2-Large | 335.3M | [Download](https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true) |
| Depth-Anything-V2-Giant | 1.3B | Coming soon |

---

## åŠŸèƒ½è©³ç´°èªªæ˜ (Detailed Features)

æœ¬å°ˆæ¡ˆæä¾›å®Œæ•´çš„æ·±åº¦ä¼°è¨ˆè§£æ±ºæ–¹æ¡ˆï¼Œä»¥ä¸‹åˆ—å‡ºæ‰€æœ‰å¯ç”¨åŠŸèƒ½ï¼š

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ä¸€è¦½

| åŠŸèƒ½ | è…³æœ¬ | èªªæ˜ |
|------|------|------|
| **åœ–ç‰‡æ·±åº¦ä¼°è¨ˆ** | `run.py` | å°å–®å¼µæˆ–å¤šå¼µåœ–ç‰‡é€²è¡Œç›¸å°æ·±åº¦ä¼°è¨ˆ |
| **å½±ç‰‡æ·±åº¦ä¼°è¨ˆ** | `run_video.py` | å°å½±ç‰‡é€å¹€é€²è¡Œæ·±åº¦ä¼°è¨ˆï¼Œè¼¸å‡ºæ·±åº¦å½±ç‰‡ |
| **å…¬åˆ¶æ·±åº¦ä¼°è¨ˆ** | `metric_depth/run.py` | è¼¸å‡ºçœŸå¯¦è·é›¢ï¼ˆå…¬å°ºï¼‰ï¼Œé©ç”¨æ–¼å®¤å…§/å®¤å¤–å ´æ™¯ |
| **é»é›²ç”Ÿæˆ** | `metric_depth/depth_to_pointcloud.py` | å°‡ 2D åœ–ç‰‡è½‰æ›ç‚º 3D é»é›² (PLY æ ¼å¼) |
| **äº’å‹•å¼ Demo** | `app.py` | åŸºæ–¼ Gradio çš„ç¶²é ä»‹é¢ï¼Œæ”¯æ´å³æ™‚é è¦½ |
| **ç›¸å°æ·±åº¦æ¸¬è©¦** | `test_depth.py` | å¿«é€Ÿæ¸¬è©¦ç›¸å°æ·±åº¦æ¨è«–ï¼ŒåŒ…å«æ•ˆèƒ½è¨ˆæ™‚ |
| **å…¬åˆ¶æ·±åº¦æ¸¬è©¦** | `test_metric.py` | å¿«é€Ÿæ¸¬è©¦å…¬åˆ¶æ·±åº¦æ¨è«–ï¼Œè¼¸å‡ºå¯¦éš›è·é›¢å€¼ |

---

### ğŸ“· 1. åœ–ç‰‡ç›¸å°æ·±åº¦ä¼°è¨ˆ (`run.py`)

å¾å–®å¼µåœ–ç‰‡é æ¸¬æ¯å€‹åƒç´ çš„ç›¸å°æ·±åº¦å€¼ï¼Œè¼¸å‡ºè¦–è¦ºåŒ–æ·±åº¦åœ–ã€‚

**è¼¸å…¥æ ¼å¼ï¼š**
- å–®å¼µåœ–ç‰‡æª”æ¡ˆ (`.jpg`, `.png` ç­‰)
- åœ–ç‰‡è³‡æ–™å¤¾
- åŒ…å«åœ–ç‰‡è·¯å¾‘çš„ `.txt` æ–‡å­—æª”

**è¼¸å‡ºæ ¼å¼ï¼š**
- å½©è‰²æ·±åº¦åœ–ï¼ˆä½¿ç”¨ Spectral_r è‰²è¡¨ï¼‰
- ç°éšæ·±åº¦åœ–ï¼ˆä½¿ç”¨ `--grayscale` é¸é …ï¼‰
- å¯é¸æ“‡åªè¼¸å‡ºæ·±åº¦åœ–æˆ–åŸåœ–+æ·±åº¦åœ–ä¸¦æ’

**åƒæ•¸èªªæ˜ï¼š**

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `--encoder` | æ¨¡å‹å¤§å°ï¼š`vits`, `vitb`, `vitl`, `vitg` | `vitl` |
| `--img-path` | è¼¸å…¥åœ–ç‰‡è·¯å¾‘ | (å¿…å¡«) |
| `--outdir` | è¼¸å‡ºç›®éŒ„ | `./vis_depth` |
| `--input-size` | æ¨è«–æ™‚çš„è¼¸å…¥å°ºå¯¸ | `518` |
| `--pred-only` | åƒ…è¼¸å‡ºæ·±åº¦åœ– | `False` |
| `--grayscale` | è¼¸å‡ºç°éšæ·±åº¦åœ– | `False` |

---

### ğŸ¬ 2. å½±ç‰‡æ·±åº¦ä¼°è¨ˆ (`run_video.py`)

å°å½±ç‰‡é€²è¡Œé€å¹€æ·±åº¦ä¼°è¨ˆï¼Œç”Ÿæˆæ·±åº¦è¦–è¦ºåŒ–å½±ç‰‡ã€‚

**è¼¸å…¥æ ¼å¼ï¼š**
- å–®å€‹å½±ç‰‡æª”æ¡ˆ (`.mp4`, `.avi` ç­‰)
- å½±ç‰‡è³‡æ–™å¤¾
- åŒ…å«å½±ç‰‡è·¯å¾‘çš„ `.txt` æ–‡å­—æª”

**è¼¸å‡ºæ ¼å¼ï¼š**
- `.mp4` æ ¼å¼çš„æ·±åº¦å½±ç‰‡
- å¯é¸æ“‡åŸå½±ç‰‡+æ·±åº¦å½±ç‰‡å·¦å³ä¸¦æ’ï¼Œæˆ–åƒ…æ·±åº¦å½±ç‰‡

**åƒæ•¸èªªæ˜ï¼š**

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `--encoder` | æ¨¡å‹å¤§å° | `vitl` |
| `--video-path` | è¼¸å…¥å½±ç‰‡è·¯å¾‘ | (å¿…å¡«) |
| `--outdir` | è¼¸å‡ºç›®éŒ„ | `./vis_video_depth` |
| `--input-size` | æ¨è«–æ™‚çš„è¼¸å…¥å°ºå¯¸ | `518` |
| `--pred-only` | åƒ…è¼¸å‡ºæ·±åº¦å½±ç‰‡ | `False` |
| `--grayscale` | è¼¸å‡ºç°éšæ·±åº¦ | `False` |

---

### ğŸ“ 3. å…¬åˆ¶æ·±åº¦ä¼°è¨ˆ (`metric_depth/`)

èˆ‡ç›¸å°æ·±åº¦ä¸åŒï¼Œå…¬åˆ¶æ·±åº¦ä¼°è¨ˆå¯è¼¸å‡ºçœŸå¯¦çš„ç‰©ç†è·é›¢ï¼ˆå–®ä½ï¼šå…¬å°ºï¼‰ã€‚

**å ´æ™¯é¡å‹ï¼š**

| å ´æ™¯ | è³‡æ–™é›† | æœ€å¤§æ·±åº¦ | é©ç”¨æƒ…å¢ƒ |
|------|--------|----------|----------|
| å®¤å…§ | Hypersim | 20 å…¬å°º | æˆ¿é–“ã€è¾¦å…¬å®¤ã€å»ºç¯‰å…§éƒ¨ |
| å®¤å¤– | Virtual KITTI | 80 å…¬å°º | è¡—é“ã€é“è·¯ã€æˆ¶å¤–ç’°å¢ƒ |

**è¼¸å‡ºæ ¼å¼ï¼š**
- æ·±åº¦è¦–è¦ºåŒ–åœ–ç‰‡ (PNG)
- åŸå§‹æ·±åº¦æ•¸æ“š (NumPy `.npy` æ ¼å¼)

**å¿«é€Ÿæ¸¬è©¦è…³æœ¬ `test_metric.py` è¼¸å‡ºç¯„ä¾‹ï¼š**
```
ã€çœŸå¯¦è·é›¢æ•¸æ“š (å–®ä½: å…¬å°º)ã€‘
  - æœ€è¿‘è·é›¢: 1.23 m
  - æœ€é è·é›¢: 15.67 m
  - ä¸­å¿ƒé»è·é›¢: 8.45 m
```

---

### ğŸŒ 4. é»é›²ç”Ÿæˆ (`metric_depth/depth_to_pointcloud.py`)

å°‡ 2D åœ–ç‰‡è½‰æ›ç‚º 3D é»é›²ï¼Œå¯ç”¨æ–¼ 3D è¦–è¦ºåŒ–å’Œå»ºæ¨¡ã€‚

**è¼¸å‡ºæ ¼å¼ï¼š**
- `.ply` é»é›²æª”æ¡ˆï¼ˆåŒ…å«é¡è‰²è³‡è¨Šï¼‰
- å¯ä½¿ç”¨ Open3Dã€MeshLab ç­‰å·¥å…·æª¢è¦–

**åƒæ•¸èªªæ˜ï¼š**

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `--encoder` | æ¨¡å‹å¤§å° | `vitl` |
| `--load-from` | æ¨¡å‹æ¬Šé‡è·¯å¾‘ | (å¿…å¡«) |
| `--max-depth` | æœ€å¤§æ·±åº¦å€¼ | `20` |
| `--img-path` | è¼¸å…¥åœ–ç‰‡è·¯å¾‘ | (å¿…å¡«) |
| `--outdir` | è¼¸å‡ºç›®éŒ„ | `./vis_pointcloud` |
| `--focal-length-x` | X è»¸ç„¦è· | `470.4` |
| `--focal-length-y` | Y è»¸ç„¦è· | `470.4` |

---

### ğŸ–¥ï¸ 5. Gradio äº’å‹•å¼ Demo (`app.py`)

æä¾›åŸºæ–¼ç¶²é çš„äº’å‹•å¼æ·±åº¦ä¼°è¨ˆç•Œé¢ã€‚

**åŠŸèƒ½ç‰¹è‰²ï¼š**
- ğŸ–¼ï¸ ä¸Šå‚³åœ–ç‰‡å³æ™‚é è¦½æ·±åº¦åœ–
- ğŸšï¸ æ»‘æ¡¿æ¯”è¼ƒåŸåœ–èˆ‡æ·±åº¦åœ–
- ğŸ“¥ ä¸‹è¼‰ç°éšæ·±åº¦åœ– (PNG)
- ğŸ“¥ ä¸‹è¼‰ 16-bit åŸå§‹æ·±åº¦æ•¸æ“š (å¯è¦–ç‚ºè¦–å·®åœ–)

---

### ğŸ§ª 6. æ¸¬è©¦è…³æœ¬

#### `test_depth.py` - ç›¸å°æ·±åº¦æ¸¬è©¦
- å¿«é€Ÿæ¸¬è©¦å–®å¼µåœ–ç‰‡çš„ç›¸å°æ·±åº¦æ¨è«–
- è¼¸å‡ºæ·±åº¦åœ–çµ±è¨ˆè³‡è¨Šï¼ˆæœ€å¤§å€¼ã€æœ€å°å€¼ï¼‰
- è¼¸å‡ºæ¨è«–æ™‚é–“æ•ˆèƒ½æ¸¬è©¦çµæœ
- å„²å­˜ NPY åŸå§‹æ•¸æ“šå’Œè¦–è¦ºåŒ–åœ–ç‰‡

#### `test_metric.py` - å…¬åˆ¶æ·±åº¦æ¸¬è©¦
- å¿«é€Ÿæ¸¬è©¦å…¬åˆ¶æ·±åº¦æ¨è«–
- æ”¯æ´å®¤å…§ (indoor) å’Œå®¤å¤– (outdoor) å ´æ™¯åˆ‡æ›
- è¼¸å‡ºçœŸå¯¦è·é›¢æ•¸æ“šï¼ˆå–®ä½ï¼šå…¬å°ºï¼‰
- åŒ…å«æ•ˆèƒ½è¨ˆæ™‚åŠŸèƒ½

---

### ğŸ”§ æ”¯æ´çš„ç¡¬é«”å¹³å°

| å¹³å° | æ”¯æ´ç‹€æ…‹ |
|------|----------|
| NVIDIA GPU (CUDA) | âœ… å®Œæ•´æ”¯æ´ |
| Apple Silicon (MPS) | âœ… æ”¯æ´ |
| CPU | âœ… æ”¯æ´ï¼ˆè¼ƒæ…¢ï¼‰ |

---

### ğŸ“¦ æ¨¡å‹è¦æ ¼æ¯”è¼ƒ

| æ¨¡å‹ | åƒæ•¸é‡ | æ¨è«–é€Ÿåº¦ | ç²¾ç¢ºåº¦ | é©ç”¨å ´æ™¯ |
|------|--------|----------|--------|----------|
| Small (vits) | 24.8M | æœ€å¿« | ä¸€èˆ¬ | å³æ™‚æ‡‰ç”¨ã€é‚Šç·£è£ç½® |
| Base (vitb) | 97.5M | å¿« | è‰¯å¥½ | å¹³è¡¡æ•ˆèƒ½èˆ‡ç²¾åº¦ |
| Large (vitl) | 335.3M | ä¸­ç­‰ | å„ªç§€ | é«˜å“è³ªæ·±åº¦ä¼°è¨ˆ |
| Giant (vitg) | 1.3B | è¼ƒæ…¢ | æœ€ä½³ | ç ”ç©¶ã€æœ€é«˜å“è³ªéœ€æ±‚ |

---

## Usage

### Prepraration

```bash
git clone https://github.com/DepthAnything/Depth-Anything-V2
cd Depth-Anything-V2
pip install -r requirements.txt
```

Download the checkpoints listed [here](#pre-trained-models) and put them under the `checkpoints` directory.

### Use our models
```python
import cv2
import torch

from depth_anything_v2.dpt import DepthAnythingV2

DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'

model_configs = {
    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
}

encoder = 'vitl' # or 'vits', 'vitb', 'vitg'

model = DepthAnythingV2(**model_configs[encoder])
model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))
model = model.to(DEVICE).eval()

raw_img = cv2.imread('your/image/path')
depth = model.infer_image(raw_img) # HxW raw depth map in numpy
```

If you do not want to clone this repository, you can also load our models through [Transformers](https://github.com/huggingface/transformers/). Below is a simple code snippet. Please refer to the [official page](https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2) for more details.

- Note 1: Make sure you can connect to Hugging Face and have installed the latest Transformers.
- Note 2: Due to the [upsampling difference](https://github.com/huggingface/transformers/pull/31522#issuecomment-2184123463) between OpenCV (we used) and Pillow (HF used), predictions may differ slightly. So you are more recommended to use our models through the way introduced above.
```python
from transformers import pipeline
from PIL import Image

pipe = pipeline(task="depth-estimation", model="depth-anything/Depth-Anything-V2-Small-hf")
image = Image.open('your/image/path')
depth = pipe(image)["depth"]
```

### Running script on *images*

```bash
python run.py \
  --encoder <vits | vitb | vitl | vitg> \
  --img-path <path> --outdir <outdir> \
  [--input-size <size>] [--pred-only] [--grayscale]
```
Options:
- `--img-path`: You can either 1) point it to an image directory storing all interested images, 2) point it to a single image, or 3) point it to a text file storing all image paths.
- `--input-size` (optional): By default, we use input size `518` for model inference. ***You can increase the size for even more fine-grained results.***
- `--pred-only` (optional): Only save the predicted depth map, without raw image.
- `--grayscale` (optional): Save the grayscale depth map, without applying color palette.

For example:
```bash
python run.py --encoder vitl --img-path assets/examples --outdir depth_vis
```

### Running script on *videos*

```bash
python run_video.py \
  --encoder <vits | vitb | vitl | vitg> \
  --video-path assets/examples_video --outdir video_depth_vis \
  [--input-size <size>] [--pred-only] [--grayscale]
```

***Our larger model has better temporal consistency on videos.***

### Gradio demo

To use our gradio demo locally:

```bash
python app.py
```

You can also try our [online demo](https://huggingface.co/spaces/Depth-Anything/Depth-Anything-V2).

***Note: Compared to V1, we have made a minor modification to the DINOv2-DPT architecture (originating from this [issue](https://github.com/LiheYoung/Depth-Anything/issues/81)).*** In V1, we *unintentionally* used features from the last four layers of DINOv2 for decoding. In V2, we use [intermediate features](https://github.com/DepthAnything/Depth-Anything-V2/blob/2cbc36a8ce2cec41d38ee51153f112e87c8e42d8/depth_anything_v2/dpt.py#L164-L169) instead. Although this modification did not improve details or accuracy, we decided to follow this common practice.


## Fine-tuned to Metric Depth Estimation

Please refer to [metric depth estimation](./metric_depth).


## DA-2K Evaluation Benchmark

Please refer to [DA-2K benchmark](./DA-2K.md).


## Community Support

**We sincerely appreciate all the community support for our Depth Anything series. Thank you a lot!**

- Apple Core ML:
    - https://developer.apple.com/machine-learning/models
    - https://huggingface.co/apple/coreml-depth-anything-v2-small
    - https://huggingface.co/apple/coreml-depth-anything-small
- Transformers:
    - https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2
    - https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything
- TensorRT:
    - https://github.com/spacewalk01/depth-anything-tensorrt
    - https://github.com/zhujiajian98/Depth-Anythingv2-TensorRT-python
- ONNX: https://github.com/fabio-sim/Depth-Anything-ONNX
- ComfyUI: https://github.com/kijai/ComfyUI-DepthAnythingV2
- Transformers.js (real-time depth in web): https://huggingface.co/spaces/Xenova/webgpu-realtime-depth-estimation
- Android:
  - https://github.com/shubham0204/Depth-Anything-Android
  - https://github.com/FeiGeChuanShu/ncnn-android-depth_anything


## Acknowledgement

We are sincerely grateful to the awesome Hugging Face team ([@Pedro Cuenca](https://huggingface.co/pcuenq), [@Niels Rogge](https://huggingface.co/nielsr), [@Merve Noyan](https://huggingface.co/merve), [@Amy Roberts](https://huggingface.co/amyeroberts), et al.) for their huge efforts in supporting our models in Transformers and Apple Core ML.

We also thank the [DINOv2](https://github.com/facebookresearch/dinov2) team for contributing such impressive models to our community.


## LICENSE

Depth-Anything-V2-Small model is under the Apache-2.0 license. Depth-Anything-V2-Base/Large/Giant models are under the CC-BY-NC-4.0 license.


## Citation

If you find this project useful, please consider citing:

```bibtex
@article{depth_anything_v2,
  title={Depth Anything V2},
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  journal={arXiv:2406.09414},
  year={2024}
}

@inproceedings{depth_anything_v1,
  title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, 
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  booktitle={CVPR},
  year={2024}
}
```
